{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEes4x4IDiLLkBR7+4qXyf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhinavarorags/CoolStuff/blob/test/QuantHW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "m2NmJUYPyTNn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "586690c7-6d87-4049-a57f-c44759af29de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stuff\n"
          ]
        }
      ],
      "source": [
        "print(\"stuff\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "url = 'https://raw.githubusercontent.com/abhinavarorags/CoolStuff/refs/heads/test/s.csv'\n",
        "url = 'https://raw.githubusercontent.com/abhinavarorags/CoolStuff/refs/heads/test/sample_data.csv'\n",
        "\n",
        "try:\n",
        "  df = pd.read_csv(url, sep=',')\n",
        "except pd.errors.ParserError as e:\n",
        "    print(f\"Error reading CSV: {e}\")\n",
        "df.head()\n",
        "print(f\"Len: {len(df)}, Shape: {df.shape[0]}, Index Size: {df.index.size}, Size/Columns: {df.size // df.columns.size}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9W2RqVz3yowG",
        "outputId": "c3b073c9-507f-4adf-92e1-3cb092d1fcf5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Len: 67048, Shape: 67048, Index Size: 67048, Size/Columns: 67048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_training = df.iloc[:20000]  # First 100 rows\n",
        "train_data = df_training\n",
        "df_test = df.iloc[20001:]     # Rows from index 100 onwards"
      ],
      "metadata": {
        "id": "RMiOpn5h6wdL"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DO NOT RUN THIS CELL\n",
        "#This is the REAL Q-learning code but it runs slower\n",
        "#Also needs lots of GPU power\n",
        "#See image asking for 84 hours of runtime\n",
        "#https://github.com/abhinavarorags/CoolStuff/blob/test/QuantFury%20Q-learning%20Compute.png\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Using the previously loaded train_data from '/mnt/data/training.csv'\n",
        "\n",
        "# Q-learning parameters\n",
        "initial_alpha = 0.5  # Initial learning rate\n",
        "min_alpha = 0.01  # Minimum learning rate\n",
        "alpha_decay = 0.995  # Decay rate for alpha\n",
        "\n",
        "initial_epsilon = 1.0  # Initial exploration rate\n",
        "min_epsilon = 0.01  # Minimum exploration rate\n",
        "epsilon_decay = 0.995  # Decay rate for epsilon\n",
        "\n",
        "gamma = 0.95  # Discount factor\n",
        "num_episodes = 1000  # Number of episodes\n",
        "\n",
        "# Define actions\n",
        "ACTIONS = {0: 'Buy Long', 1: 'Sell Short', 2: 'Hold'}\n",
        "\n",
        "# Initialize Q-table\n",
        "q_table = np.zeros((len(train_data), len(ACTIONS)))\n",
        "\n",
        "# Function to choose an action using epsilon-greedy strategy\n",
        "def choose_action(state_index, epsilon):\n",
        "    if random.uniform(0, 1) < epsilon:\n",
        "        return random.choice(list(ACTIONS.keys()))  # Explore\n",
        "    else:\n",
        "        return np.argmax(q_table[state_index])  # Exploit\n",
        "\n",
        "# Q-learning process\n",
        "for episode in range(num_episodes):\n",
        "    alpha = max(min_alpha, initial_alpha * (alpha_decay ** episode))\n",
        "    epsilon = max(min_epsilon, initial_epsilon * (epsilon_decay ** episode))\n",
        "    total_reward = 0\n",
        "\n",
        "    for state_index in range(len(train_data) - 1):\n",
        "        current_state = train_data.iloc[state_index].values[:-1]  # Exclude Date\n",
        "        action = choose_action(state_index, epsilon)\n",
        "        reward = train_data.iloc[state_index + 1]['Close'] - train_data.iloc[state_index]['Close']\n",
        "        next_state_index = state_index + 1\n",
        "        best_future_q = np.max(q_table[next_state_index]) if next_state_index < len(train_data) else 0\n",
        "        q_table[state_index, action] += alpha * (reward + gamma * best_future_q - q_table[state_index, action])\n",
        "        total_reward += reward\n",
        "\n",
        "    if episode % 100 == 0:\n",
        "        print(f\"Episode: {episode}, Alpha: {alpha:.4f}, Epsilon: {epsilon:.4f}, Total Reward: {total_reward}\")\n",
        "\n",
        "# Save the Q-table to CSV\n",
        "q_table_df = pd.DataFrame(q_table, columns=ACTIONS.keys())\n",
        "#q_table_df.to_csv('/mnt/data/q_table.csv', index=False)\n",
        "print(\"Q-table training complete and saved as q_table.csv.\")\n"
      ],
      "metadata": {
        "id": "F9EGZ2k3FOYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dumbed down the code to only 'Close' Prices and 10 episodes\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Using the previously loaded train_data from '/mnt/data/training.csv'\n",
        "\n",
        "# Q-learning parameters\n",
        "initial_alpha = 0.5  # Initial learning rate\n",
        "min_alpha = 0.01  # Minimum learning rate\n",
        "alpha_decay = 0.995  # Decay rate for alpha\n",
        "\n",
        "initial_epsilon = 1.0  # Initial exploration rate\n",
        "min_epsilon = 0.01  # Minimum exploration rate\n",
        "epsilon_decay = 0.995  # Decay rate for epsilon\n",
        "\n",
        "gamma = 0.95  # Discount factor\n",
        "\n",
        "# Convert necessary columns to NumPy arrays for faster access\n",
        "close_prices = train_data['Close'].values\n",
        "actions = np.array(list(ACTIONS.keys()))\n",
        "\n",
        "# Reduce the number of episodes for demonstration\n",
        "num_episodes = 10  # Use a smaller number for quicker demonstration\n",
        "\n",
        "# Initialize Q-table\n",
        "q_table = np.zeros((len(close_prices) - 1, len(actions)))\n",
        "\n",
        "# Run the Q-learning algorithm with optimized data handling\n",
        "for episode in range(num_episodes):\n",
        "    alpha = max(min_alpha, initial_alpha * (alpha_decay ** episode))\n",
        "    epsilon = max(min_epsilon, initial_epsilon * (epsilon_decay ** episode))\n",
        "    total_reward = 0\n",
        "\n",
        "    for i in range(len(close_prices) - 1):\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = random.choice(actions)\n",
        "        else:\n",
        "            action = actions[np.argmax(q_table[i])]\n",
        "\n",
        "        reward = close_prices[i + 1] - close_prices[i]\n",
        "        next_state_action_values = q_table[i + 1] if i + 1 < len(close_prices) - 1 else np.zeros(len(actions))\n",
        "        best_future_q = np.max(next_state_action_values)\n",
        "        q_table[i, action] += alpha * (reward + gamma * best_future_q - q_table[i, action])\n",
        "        total_reward += reward\n",
        "\n",
        "    print(f\"Episode: {episode}, Alpha: {alpha:.4f}, Epsilon: {epsilon:.4f}, Total Reward: {total_reward}\")\n",
        "\n",
        "# Convert Q-table to DataFrame and save to CSV\n",
        "q_table_df = pd.DataFrame(q_table, columns=[str(action) for action in actions])\n",
        "#q_table_df.to_csv('/mnt/data/q_table_optimized.csv', index=False)\n",
        "print(\"Optimized Q-table training complete and saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4NYHCkKFjCP",
        "outputId": "d9b340f6-5a3b-44b3-fbd0-78254ea2a036"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0, Alpha: 0.5000, Epsilon: 1.0000, Total Reward: 173.41\n",
            "Episode: 1, Alpha: 0.4975, Epsilon: 0.9950, Total Reward: 173.41\n",
            "Episode: 2, Alpha: 0.4950, Epsilon: 0.9900, Total Reward: 173.41\n",
            "Episode: 3, Alpha: 0.4925, Epsilon: 0.9851, Total Reward: 173.41\n",
            "Episode: 4, Alpha: 0.4901, Epsilon: 0.9801, Total Reward: 173.41\n",
            "Episode: 5, Alpha: 0.4876, Epsilon: 0.9752, Total Reward: 173.41\n",
            "Episode: 6, Alpha: 0.4852, Epsilon: 0.9704, Total Reward: 173.41\n",
            "Episode: 7, Alpha: 0.4828, Epsilon: 0.9655, Total Reward: 173.41\n",
            "Episode: 8, Alpha: 0.4803, Epsilon: 0.9607, Total Reward: 173.41\n",
            "Episode: 9, Alpha: 0.4779, Epsilon: 0.9559, Total Reward: 173.41\n",
            "Optimized Q-table training complete and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oxk8mPT9HdVc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}